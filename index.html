<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Susana Zoghbi | Website</title>
  <link rel="stylesheet" href="css/bootstrap.min.css" crossorigin="anonymous">
  <link rel="stylesheet" href="css/style.css" crossorigin="anonymous">
  <link href='https://fonts.googleapis.com/css?family=Roboto:400,300,500' rel='stylesheet' type='text/css'>

  <!-- Tracking code -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-24159375-5"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-24159375-5');
  </script>


</head>


<body>

  <!-- Header -->
  <div id="header" class="bg1">
    <div class="container">
      <div class="row" id="headerblob">
        <div class="col-md-4 col-sm-3" id="intro_txt">
          <div id="htname">Susana Zoghbi</div>
          <div id="htdesc">PhD in Computer Science</div>
          <div id="htem">susana _at_ macty.eu</div>
          <div id="icons">
            <div class="svgico">
            <a href="https://www.linkedin.com/in/susana-zoghbi/"><img src="images/linkedin.png" width="56px"></a>
            </div>
          </div>
        </div>
        <div class="col-md-5"></div>
        <div class="col-md-3 col-sm-3">
          <img src="images/susana2.jpg" class="img-circle imgme" width="200" height="200">
        </div>
      </div>
    </div>
  </div>

  <!-- Quote -->
  <div class="container">
    <div class="row">
      <blockquote>
        <p style="font-size:24px; font-style:italic;">I want to help machines understand the world to enable businesses growth.</p>
      </blockquote>
    </div>

  </div>

  <!-- Horizontal Line -->
  <!-- <hr class="soft"> -->

  <div class="container">
    <h2>Talks</h2>
    <hr class="soft" style="margin:5px">
    <div class="col-md-12">
      <!-- Publication Image -->
      <div class="pubimg">
        <a target="_blank" href="https://www.meteorshowers.org/view/Perseids">
        <img src="images/perseids2.png" style="padding-top: 35px; padding-bottom: 12px; margin-bottom: 10px; max-height: 300px">
        </a>
      </div>

    </div>

  </div>

  <div class="container">
      <h2>Publications</h2>
      <h4>(Also on <a href="https://scholar.google.com/citations?hl=en&user=ravmkhkAAAAJ" target="_blank">Google Scholar</a>)</h4>

      <hr class="soft" style="margin:5px">

      <!-- ------------------------- -->
      <!-- New Publication Entry -->
      <div class="pubwrap">
        <div class="row">
          <div class="col-md-12">
              <div class="pub_title">Searching for Long-Period Comets with Deep Learning </div>
              <br>
              <!-- Sponsored By -->
              <div class="row">
                <div class="col-md-12">
                  <p>Sponsored by:
                  <img src="images/nasa_logo.jpg" height="30" alt="Nasa" style='margin: auto;'>
                  <img src="images/seti_logo.png" height="30" alt="seti" style='margin: auto;'>
                  <img src="images/ibm_logo.png" height="30" alt="ibm" style='margin: auto;'>
                  <img src="images/nvidia.png" height="30" alt="nvidia" style='margin: auto;'>
                  </p>
                </div>
              </div> <!-- end of row -->

              <div class="pub_description">
                <div class="row">
                  <div class="col-md-12">
                    In planetary defense, long-period comets are recognized as the potentially most devastating threat. However, any new comet on an impact trajectory would likely only be discovered about one year before impact. The goal of this project is to add years of extra warning time by providing comet searchers directions on where to look for comets when they are still far out. To aid and guide a dedicated search for these dangerous objects, meteor showers may offer a clue. Comets leave debris trails as they travel along their orbits. When our planet intersects such debris trails, we see them as meteors. By detecting rare aperiodic meteor showers from dust clouds, we can estimate the orbit of the parent body and narrow down the search space where to look for long-period comets.
                  </div>

                  <div class="col-md-12">
                    <!-- Publication Image -->
                    <div class="pubimg">
                      <a target="_blank" href="https://www.meteorshowers.org/view/Perseids">
                      <img src="images/perseids2.png" style="padding-top: 35px; padding-bottom: 12px; margin-bottom: 10px; max-height: 300px">
                      </a>
                    </div>

                    <div>
                      <p class="imgcaption" style="font-size:14px">Illustration of debris trail from the comet Swift-Tuttle. When Earth passes through this trail, we see the famous Perseid meteor shower.</p>
                    </div>

                    <br><br><br><br>
                    The Cameras for Allsky Meteor Surveillance or CAMS monitors the sky to detect meteors. Until now, processing the images has required time-consuming human input to rule out false positives. Automating this process allows to free the data analyst in CAMS and enable a global expansion and temporal coverage of the camera network that can detect the dust trails of those potentially hazardous long period comets that came close to Earth’s orbit in the past ten millennia.
                    <br>
                    We developed deep learning tools that allow such automation. Specifically, we developed a Convolutional Neural Network (CNN) that discerns images of meteors vs. other objects in the sky and achieves precision and recall scores of 88.3% and 90.3%, respectively.  In addition, we developed a Long-Short Term Memory (LSTM) network that encodes the light curve tracklets into a latent space, and learns to predict whether the tracklet corresponds to a meteor or not. The LSTM achieves a precision of 90.0% and a recall of 89.1%.  These methods can now be used by meteor astronomers to automatically analyze sky detections and help guide the search for long-period comets.
                  </div>
                </div>  <!-- end of row -->
              </div> <!-- end of pub_description -->

            <div class="pub_authors">
              Susana Zoghbi, Marcelo De Cicco, Antonio Ordoñez, Andres Plata Stapper, Peter S. Gural, Siddha Ganju, and Peter Jenniskens
            </div>

            <div class="pub_venue">
              Workshop on Deep Learning for Physical Sciences (DLPS 2017), NIPS 2017, Long Beach, CA, USA.
            </div>
              <!-- External Links -->
            <div>
              <ul>
                <li>
                <a href="https://dl4physicalsciences.github.io/files/nips_dlps_2017_23.pdf" target="_blank" class="btn btn-info" role="button">PDF</a>
                </li>
                <li>
                <a href="http://comets.frontierdevelopmentlab.org/#/mission" target="_blank" class="btn btn-info" role="button">Mission</a>
                </li>
                <li>
                <a href="http://cams.seti.org" target="_blank" class="btn btn-info" role="button">CAMS</a>
                </li>
              </ul>
            </div>
            <!-- end external links -->
          </div>
        </div>
      </div> <!-- end pubwrap -->

      <!-- ------------------------- -->
      <!-- New Publication Entry -->
      <!-- ------------------------- -->
      <div class="pubwrap">
        <div class="row">


          <div class="pub">
            <div class="col-md-12">
              <div class="pub_title">Latent Dirichlet Allocation for Linking User-Generated Content and e-Commerce Data</div>
            </div>

            <div class="col-md-6">
              <div class="pubd">
                Automatic linking of online content improves navigation possibilities for end users. We focus on linking content generated by users to other relevant sites. In particular, we study the problem of linking information between different usages of the same language, e.g., colloquial and formal <i>idioms</i> or the language of consumers versus the language of sellers. The challenge is that the same items are described using very distinct vocabularies. As a case study, we investigate a new task of linking textual Pinterest.com pins (colloquial) to online webshops (formal).

                We evaluate three different modeling paradigms based on probabilistic topic modeling: monolingual latent Dirichlet allocation (LDA),  bilingual LDA (BiLDA) and a novel multi-idiomatic LDA model (MiLDA). We compare these to the unigram model with Dirichlet prior.  Our results for all three topic models reveal the usefulness of modeling the hidden thematic structure of the data through topics. Our proposed MiLDA model is able to deal with intrinsic multi-idiomatic data by considering the shared vocabulary between the aligned document pairs.

              </div>
              <div class="pub_authors">Susana Zoghbi, Ivan Vulic, Sien Moens</div>
              <div class="pub_venue">Information Sciences, 2016</div>
              <div>
                <ul>
                  <li>
                  <a href="myPublications/inf_science_main.pdf" target="_blank" class="btn btn-info" role="button">PDF</a>
                  </li>
                </ul>
              </div>
            </div>

            <div class="col-md-6">
              <div class="pubimg">
                <img src="images/sigir_img.png" style="padding-bottom: 30px">
              </div>
              <div>
                <p class="imgcaption" style="font-size: 15px;">Graphical representation of the Multiidiomatic Latent Dirichlet Allocation model</p>
              </div>
            </div>
          </div>

        </div>
      </div>

      <!-- ------------------------- -->
      <!-- New Publication Entry -->
      <!-- ------------------------- -->

      <div class="pubwrap">
            <div class="row">

              <div class="col-md-6">
                <div class="pub">
                  <div class="pub_title">Learning to Bridge Colloquial and Formal Language Applied to Linking and Search of E-Commerce Data</div>

                  <div class="pubd">
                    We study the problem of linking information between different idiomatic usages of the same language, for example, colloquial and formal language. We propose a novel probabilistic topic model called multi-idiomatic LDA (MiLDA). Its modeling principles follow the intuition that certain words are shared between two idioms of the same language, while other words are non-shared. We demonstrate the ability of our model to learn relations between cross-idiomatic topics in a dataset containing product descriptions and reviews. We present the utility of the new MiLDA topic model in a recently proposed information retrieval task of linking Pinterest pins to online webshops . We show that our multi-idiomatic model outperforms the standard monolingual LDA model and the pure bilingual LDA model both in terms of perplexity and MAP scores in the IR task.

                  </div>
                  <div class="pub_authors">Ivan Vulic, Susana Zoghbi and Sien Moens</div>
                  <div class="pub_venue">
                    ACM SIGIR Conference on Research & Development in Information Retrieval (SIGIR '14)
                  </div>
                  <div>
                    <ul>
                      <li>
                      <a href="myPublications/sigir_2014.pdf"  target="_blank" class="btn btn-info" role="button">PDF</a>
                      </li>

                    </ul>
                  </div>

                </div>

              </div>

              <div class="col-md-6">
                <div class="pubimg">
                  <img src="images/inf_science.png" style="">
                </div>
                <div>
                  <p class="imgcaption" style="padding-top: 30px; font-size:14px">Examples that may be linked between user-generated content from Pinterest.com (left) and e-commerce data from Amazon.com.  We see the difference in language usage between social media and online products.  On each row, the items are the same (or very similar), but the textual description differs. This difference in language makes it difficult to link the items as referring to the related objects.</p>
                </div>
              </div>

            </div>

      </div>

      <!-- ------------------------- -->
      <!-- New Publication Entry -->
      <!-- ------------------------- -->
      <div class="pubwrap">
        <div class="row">
          <div class="col-md-12">
            <div class="pub_title">
              Cross-modal Search for Fashion Attributes
            </div>

            <div class="pub_description">
              <div class="row">
                <div class="col-md-8">
                  In this paper we develop a neural network which learns inter- modal representations for fashion attributes to be utilized in a cross-modal search tool. Our neural network learns from organic e-commerce data, which is characterized by clean image material, but noisy and incomplete product descrip- tions. First, we experiment with techniques to segment e- commerce images and their product descriptions into respec- tively image and text fragments denoting fashion attributes. Here, we propose a rule-based image segmentation approach which exploits the cleanness of e-commerce images. Next, we design an objective function which encourages our model to induce a common embedding space where a semantically related image fragment and text fragment have a high in- ner product. This objective function incorporates similarity information of image fragments to obtain better intermodal representations. A key insight is that similar looking image fragments should be described with the same text fragments. We explicitly require this in our objective function, and as such recover information which was lost due to noise and in- completeness in the product descriptions. We evaluate the inferred intermodal representations in cross-modal search. We demonstrate that the neural network model trained with our objective function on image fragments acquired with our rule-based segmentation approach improves the results of image search with textual queries by 198% for recall@1 and by 181% for recall@5 compared to results obtained by a state-of-the-art image search system on the same benchmark dataset.
                </div>
                <div class="col-md-4">
                <img src="images/kdd.png" height="300" alt="">
                <p class="imgcaption" style="padding-top: 30px; font-size:14px">We learn to align image fragments and textual segments which improves performance for the task of cross-modal search.</p>
                </div>
                <div class="col-md-12" align="center">
                  <!-- Publication Image -->
                  <div class="pubimg">
                    <img src="images/inner_product.jpg" style="padding-top: 35px; padding-bottom: 12px; margin-bottom: 10px; max-height: 350px">
                    </a>
                  </div>

                  <div>
                    <p class="imgcaption" style="font-size:14px">Model overview. Left: An image is segmented into regions, which together with the full image
                    function as the image fragments. The image fragments are embedded through a CNN. Right: A product
                    description is filtered with a known glossary. Each phrase is a text fragment and is represented
                    with a word embedding. Middle: The alignment model learns to project semantically related image and text
                    fragments to vectors into a common embedding space which have a high inner product, as depicted by dark
                    shades of grey. The resulting intermodal representations form the core building blocks for a cross-modal
                    search tool.</p>
                  </div>
                </div>
              </div>
            </div>
            <div class="pub_authors">Katrien Laenen, Susana Zoghbi, Sien Moens</div>
              <div class="pub_venue">KDD Machine Learning Meets Fashion Worshop, 2017</div>
              <div>
                <ul>
                  <li>
                  <a href="https://kddfashion2017.mybluemix.net/final_submissions/ML4Fashion_paper_7.pdf" target="_blank" class="btn btn-info" role="button">PDF</a>
                  </li>
                </ul>
              </div>
            </div>
        </div>
      </div>





      <!-- ------------------------- -->
      <!-- New Publication Entry -->
      <!-- ------------------------- -->
      <div class="pubwrap">
            <div class="row">

              <div class="col-md-12">
                <div class="pub">
                  <div class="pub_title">Fashion Meets Computer Vision and NLP at E-Commerce
                Search</div>

                  <div class="pubd">
                    We focus on cross-modal (visual and textual) e-commerce search within the fashion domain. Particularly, we investigate two tasks: 1) given a query image, we retrieve textual descriptions that correspond to the visual attributes in the query; and 2) given a textual query that may express an interest in specific visual product characteristics, we retrieve relevant images that exhibit the required visual attributes. Our dataset consists of 53,689 images coupled with textual descriptions. The images contain fashion garments that display a great variety of visual attributes, such as different shapes, colors and textures in natural language. Unlike previous datasets, the text provides a rough and noisy description of the item in the image. We extensively analyze this dataset in the context of cross-modal e-commerce search. We investigate two latent variable models to bridge between textual and visual data: bilingual latent Dirichlet allocation and canonical correlation analysis. We use state-of-the-art visual and textual features and report promising results

                  </div>

                </div>
              </div>

          </div>


          <div class="row"  style="padding-top: 30px;">
              <div class="col-md-6">
                <div class="pubimg">
                <img src="images/img2txt.png">
                </div>
                <div>
                  <p class="imgcaption">Image to text</p>
                </div>
              </div>
              <div class="col-md-6">
                <div class="pubimg">
                  <img src="images/txt2img.png" style="padding-bottom: 30px;">
                </div>
                <div>
                  <p class="imgcaption">Text to image</p>
                </div>
              </div>
          </div>

          <div class="row">
            <div class="col-md-12">
              <div class="pub_authors">Susana Zoghbi, Geert Heyman, Juan Carlos Gomez, Sien Moens</div>
              <div class="pub_venue">International Journal of Computer and Electrical Engineering (IJCEE), 2016</div>
              <div>
                <ul>
                  <li>
                  <a href="myPublications/IJCEE_Final.pdf"  target="_blank" class="btn btn-info" role="button">PDF</a>
                  </li>
                </ul>
              </div>

            </div>

          </div>

      </div>



      <!-- ------------------------- -->
      <!-- New Publication Entry -->
      <!-- ------------------------- -->
      <div class="pubwrap">
            <div class="row">

              <div class="col-md-6">
                <div class="pub">
                  <div class="pub_title">Cross-Modal Fashion Search</div>

                  <div class="pubd">
                    In this paper we show an online demo that allows bidrectional multimodal queries for garments.

                  </div>
                  <div class="pub_authors">Susana Zoghbi, Geert Heyman, Juan Carlos Gomez, Sien Moens</div>
                  <div class="pub_venue">In Lecture Notes in Computer Science (LNCS) Vol. 9517, pp 367-373, 2016</div>
                  <div>
                    <ul>
                      <li>
                      <a href="myPublications/fashion_demo_LNCS.pdf"  target="_blank" class="btn btn-info" role="button">PDF</a>
                      </li>
                      <!-- <li>
                      <a href="video/demo.mp4" class="btn btn-info" role="button">Video</a>
                      </li> -->
                    </ul>
                  </div>
                </div>
              </div>
              <div class="col-md-6">
                <div class="pubimg">
                  <img src="images/demo_img2txt.png" style="">
                </div>

              </div>

        </div>
      </div>


      <!-- ------------------------- -->
      <!-- New Publication Entry -->
      <!-- ------------------------- -->

      <div class="pubwrap">
          <div class="row">

            <div class="col-md-6">
              <div class="pub">
                <div class="pub_title">Inferring User Interests on Social Media From Text and Images</div>

                <div class="pubd">
                  We propose to infer user interests on social media where multi-modal data (text, image etc.) exist. We leverage user-generated data from Pinterest.com as a natural expression of users’ interests. Our main contribution is exploiting a multi-modal space composed of images and text. This is a natural approach since humans express their interests with a combination of modalities.  We performed experiments using the state-of-the-art image and textual representations, such as convolutional neural networks, word embeddings, and bags of visual and textual words. Our experimental results show that in fact jointly processing image and text increases the overall interest classification accuracy, when compared to uni-modal representations (i.e., using only text or using only images).

                </div>
                <div class="pub_authors">Yagmur Gizem Cinar, Susana Zoghbi, Marie-Francine Moens</div>
                <div class="pub_venue">
                  International Workshop on Social Media Retrieval and Analysis in conjunction with the IEEE International Conference on Data Mining (ICDM 2015)
                </div>
                <div>
                  <ul>
                    <li>
                    <a href="myPublications/SoMeRA_2015.pdf"  target="_blank" class="btn btn-info" role="button">PDF</a>
                    </li>
                  </ul>
                </div>

              </div>

            </div>
            <div class="col-md-6">
              <div class="pubimg">
                <img src="images/somera.png" style="padding-bottom: 30px">
              </div>
              <div>
                <p class="imgcaption" style="font-size: 15px;">Examples of pins (image-text pairs) with the corresponding categories (top) our system predicts</p>
              </div>
            </div>

          </div>

        </div>



        <!-- ------------------------- -->
        <!-- New Publication Entry -->
        <!-- ------------------------- -->

      <div class="pubwrap">
            <div class="row">
              <div class="col-md-8">
                <div class="pub">
                  <div class="pub_title">Are words enough?: a study on text-based representations and retrieval models for linking pins to online shops.</div>
                  <div class="pub_authors">Ivan Vulic, Susana Zoghbi and Sien Moens</div>
                  <div class="pub_venue">
                    Proceedings of the 2013 International Workshop on Mining Unstructured Big Data
                using Natural Language Processing in conjunction with The 24th ACM International Conference on
                Information and Knowledge Management (CIKM 2013)
                  </div>
                  <div>
                    <ul>
                      <li>
                      <a href="https://lirias.kuleuven.be/bitstream/123456789/413290/1/ZoghbietalCIKMBIGDATA2013.pdf"  target="_blank" class="btn btn-info" role="button">PDF</a>
                      </li>
                    </ul>
                  </div>
                </div>
              </div>
        </div>
      </div>

      <div class="pubwrap">
            <div class="row">
              <div class="col-md-8">
                <div class="pub">
                  <div class="pub_title">I pinned it. where can i buy one like it?: Automatically linking
                pinterest pins to online webshops</div>
                  <div class="pub_authors">Susana Zoghbi, Ivan Vulic and Sien Moens</div>
                  <div class="pub_venue">
                    DUBMOD '13 Proceedings of the 2013 workshop on Data-driven User Behavioral
                Modelling and Mining from Social Media.
                Pages 9-12. In conjunction with The 24th ACM International Conference on Information and Knowledge
                Management (CIKM 2013)
                  </div>
                  <div>
                    <ul>
                      <li>
                      <a href="myPublications/p9_zoghbi.pdf"  target="_blank" class="btn btn-info" role="button">PDF</a>
                      </li>
                    </ul>
                  </div>
                </div>
              </div>
        </div>
      </div>

      <div class="pubwrap">
            <div class="row">
              <div class="col-md-8">
                <div class="pub">
                  <div class="pub_title">Recognising personality traits using Facebook status
                updates</div>
                  <div class="pub_authors">Golnoosh Farnadi, Susana Zoghbi, Marie-Francine Moens, Martine De Cock</div>
                  <div class="pub_venue">
                    Workshop on Computational Personality Recognition (WCPR 2013) in conjunction with
                the Seventh International AAAI Conference on Weblogs and Social Media (ICWSM)
                  </div>
                  <div>
                    <ul>
                      <li>
                      <a href="myPublications/farnadi_wcpr13.pdf" target="_blank" class="btn btn-info" role="button">PDF</a>
                      </li>
                    </ul>
                  </div>
                </div>
              </div>
        </div>
      </div>

      <div class="pubwrap">
            <div class="row">
              <div class="col-md-8">
                <div class="pub">
                  <div class="pub_title">How well do your facebook status updates express your personality</div>
                  <div class="pub_authors">Golnoosh Farnadi, Susana Zoghbi, Marie-Francine Moens, Martine De Cock</div>
                  <div class="pub_venue">
                    Proceedings of the 22nd Edition of the Annual Belgian-Dutch Conference on Machine
                Learning, BENELEARN 2013
                  </div>
                  <div>
                    <ul>
                      <li>
                      <a href="myPublications/benelearn.pdf" target="_blank" class="btn btn-info" role="button">PDF</a>
                      </li>
                    </ul>
                  </div>
                </div>
              </div>
        </div>
      </div>



      <div class="pubwrap">
            <div class="row">
              <div class="col-md-8">
                <div class="pub">
                  <div class="pub_title">Enhancing collaborative human–robot
                interaction through physiological-signal based communication</div>
                  <div class="pub_authors">Susana Zoghbi, Chris Parker, Elizabeth Croft and H.F. Machiel Van der Loos</div>
                  <div class="pub_venue">
                    Proceedings of Workshop on Multimodal Human–Robot Interfaces, 2010 IEEE
                International Conference on Robotics and Automation (ICRA 2010)
                  </div>
                  <div>
                    <ul>
                      <li>
                      <a href="myPublications/Zoghbi_Parker_Croft_VanDerLoosFinal.pdf" target="_blank" class="btn btn-info" role="button">PDF</a>
                      </li>
                    </ul>
                  </div>
                </div>
              </div>
        </div>
      </div>


      <div class="pubwrap">
            <div class="row">
              <div class="col-md-8">
                <div class="pub">
                  <div class="pub_title">Measuring intent in human-robot cooperative manipulation.</div>
                  <div class="pub_authors">Davide De Carli, Evan Hohert, Chris AC Parker, Susana Zoghbi, Simon Leonard,
                Elizabeth Croft, Antonio Bicchi</div>
                  <div class="pub_venue">
                    Proceedings of Workshop on Multimodal Human–Robot Interfaces, 2010 IEEE
                International Conference on Robotics and Automation (ICRA 2010)
                  </div>
                  <div>
                    <ul>
                      <li>
                      <a href="myPublications/decarli.pdf" target="_blank" class="btn btn-info" role="button">PDF</a>
                      </li>
                    </ul>
                  </div>
                </div>
              </div>
        </div>
      </div>


      <div class="pubwrap">
            <div class="row">
              <div class="col-md-8">
                <div class="pub">
                  <div class="pub_title">Measurement instruments for the anthropomorphism, animacy,
                likeability, perceived intelligence, and perceived safety of robots</div>
                  <div class="pub_authors">Shristoph Bartneck, Dana Kulić, Elizabeth Croft, Susana Zoghbi</div>
                  <div class="pub_venue">
                    International journal of social robotics, pp 71-81. (2009).
                  </div>
                  <div>
                    <ul>
                      <li>
                      <a href="myPublications/godspeed.pdf" target="_blank" class="btn btn-info" role="button">PDF</a>
                      </li>
                    </ul>
                  </div>
                </div>
              </div>
        </div>
      </div>


      <div class="pubwrap">
            <div class="row">
              <div class="col-md-8">
                <div class="pub">
                  <div class="pub_title">Evaluation of affective state estimations using an on-line reporting
                device during human-robot interactions</div>
                  <div class="pub_authors">Susana Zoghbi, Elizabeth Croft, Dana Kulić, Mike Van der Loos</div>
                  <div class="pub_venue">
                    International Conference on Intelligent Robots and Systems, 2009. IROS 2009.
                IEEE/RSJ
                  </div>
                  <div>
                    <ul>
                      <li>
                      <a href="myPublications/affect.pdf" target="_blank" class="btn btn-info" role="button">PDF</a>
                      </li>
                    </ul>
                  </div>
                </div>
              </div>
        </div>
      </div>


      <div class="pubwrap">
            <div class="row">
              <div class="col-md-8">
                <div class="pub">
                  <div class="pub_title">On line-Affective state reporting device: A tool for evaluating
                affective state inference systems</div>
                  <div class="pub_authors">Susana Zoghbi, Dana Kuliff, Elizabeth Croft, Machiel Van der Loos</div>
                  <div class="pub_venue">
                    ACM Proceedings of the 4th ACM/IEEE international conference on Human robot
                interaction, 2009.
                  </div>
                  <div>
                    <ul>
                      <li>
                      <a href="myPublications/06256079.pdf" target="_blank" class="btn btn-info" role="button">PDF</a>
                      </li>
                    </ul>
                  </div>
                </div>
              </div>
        </div>
      </div>

      </div>


</body>

</html>
