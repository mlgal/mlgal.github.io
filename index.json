





































































[{"categories":["Waste","Analytics","Object Detection","Deep Learning"],"contents":"At Greyparrot.ai, I helped build a world-class waste analytics platform. I led the Deep Learning team to create a system that automatically recognizes all trash items going through recycling facilities, helping recover all valuable materials and converting them to assets. Cameras were installed on top of conveyor belts that transport waste. Our tech accurately recognized around 50 distinct object categories and hundreds of brands, empowering robotic manipulation and data-driven insights.\nAs Head of the Deep Learning team, I managed the end-to-end process – from image intake to model creation, evaluation and deployment. I helped design and implement this process such that the full pipeline is done continuously with as little human input as possible. During my position, we had recognized billions of objects (yes, we humans create a lot of trash!), and we were able to recognize them with less than 5% error rate on each waste stream.\nHere\u0026rsquo;s a demo of our waste recognition system:\nYour browser does not support the video tag. One important challenge we overcame was reducing the need of massive amounts of high-quality annotated data for our Deep Learning models to work well. We developed an algorithm that helps the neural network identify objects that should be added to training set, so that only the most informative data is annotated. This helped us reduce annotation efforts by 100x.\nI was very proud to be able to drive such an important technology to help reduce our waste and footprint on the planet.\nHere\u0026rsquo;s a video where I explain this work further\nAlso, here\u0026rsquo;s a blog post of what a \u0026ldquo;Day in the Life\u0026rdquo; was for me as the Head of the Deep Learning team.\nAnd don\u0026rsquo;t forget to heck out the amazing work at Greyparrot.ai\n","image":"https://mlgal.github.io/images/greyparrot.jpg","permalink":"https://mlgal.github.io/blogs/post-10/","tags":["Waste","Analytics","Object Detection","Deep Learning"],"title":"Waste Analytics System for Greyparrot.ai"},{"categories":["Contrastive Learning","Object Detection","Deep Learning"],"contents":"I gave a talk about how we are using Computer Vision to detect and track all waste through recycling facilities for the Women in Data Science group.\n","image":"https://mlgal.github.io/images/talk_wids.jpg","permalink":"https://mlgal.github.io/blogs/post-11/","tags":["Talk","Contrastive Learning","Object Detection","Deep Learning"],"title":"Talk: Computer Vision for Recycling and Waste Management"},{"categories":["NASA","SETI","Image Processing","Deep Learning","Convolutional Neural Network","LSTMs"],"contents":"Together with my team, I gave a talk on how we can provide more warning time in case a long-period comet happens to be in a collision trajectory with Earth. .\n","image":"https://mlgal.github.io/images/nasa_talk.jpg","permalink":"https://mlgal.github.io/blogs/post-13/","tags":["Talk","NASA","SETI","Image Processing","Deep Learning","Convolutional Neural Network","LSTMs"],"title":"SETI Talk: Planetary Defense: Long Period Comets"},{"categories":["NASA","SETI","Image Processing","Deep Learning","Convolutional Neural Network","LSTMs"],"contents":"In planetary defense (yes! there is such a thing, where scientists and engineers try to defend the planet from hazards), long-period comets are recognized as the potentially most devastating threat. However, any new comet on an impact trajectory would likely only be discovered about one year before impact. The goal of this project is to add years of extra warning time by providing comet searchers directions on where to look for comets when they are still far out. To aid and guide a dedicated search for these dangerous objects, meteor showers may offer a clue. Comets leave debris trails as they travel along their orbits. When our planet intersects such debris trails, we see them as meteors. By detecting rare aperiodic meteor showers from dust clouds, we can estimate the orbit of the parent body and narrow down the search space where to look for long-period comets.\nThe Cameras for Allsky Meteor Surveillance or CAMS monitors the sky to detect meteors. Until now, processing the images has required time-consuming human input to rule out false positives. Automating this process allows to free the data analyst in CAMS and enable a global expansion and temporal coverage of the camera network that can detect the dust trails of those potentially hazardous long period comets that came close to Earth’s orbit in the past ten millennia. We developed deep learning tools that allow such automation. Specifically, we developed a Convolutional Neural Network (CNN) that discerns images of meteors vs. other objects in the sky and achieves precision and recall scores of 88.3% and 90.3%, respectively. In addition, we developed a Long-Short Term Memory (LSTM) network that encodes the light curve tracklets into a latent space, and learns to predict whether the tracklet corresponds to a meteor or not. The LSTM achieves a precision of 90.0% and a recall of 89.1%. These methods can now be used by meteor astronomers to automatically analyze sky detections and help guide the search for long-period comets.\nCheck out our paper Searching for Long Period Comets with Deep Learning\nWorkshop on Deep Learning for Physical Sciences (DLPS 2017), NIPS 2017, Long Beach, CA, USA.\nSusana Zoghbi, Marcelo De Cicco, Antonio Ordoñez, Andres Plata Stapper, Peter S. Gural, Siddha Ganju, and Peter Jenniskens\nPDF\n","image":"https://mlgal.github.io/images/perseids2.png","permalink":"https://mlgal.github.io/blogs/post-1/","tags":["NASA","SETI","Image Processing","Deep Learning","Convolutional Neural Network","LSTMs"],"title":"Searching for Long Period Comets with Deep Learning at NASA and SETI"},{"categories":["Chatbot","Image Processing","Image Recognition","Deep Learning","Visual Search"],"contents":"Leveraging our image and pattern recognition technology, we developed a chatbot that enables users to search product collections using images. Additionally, users can search using both language and images together. For example, I like this item, but I want it with a longer skirt and a V-neck instead of high neck.\nSee it in action here:\nYour browser does not support the video tag. From the video above, you can see you can perform the following actions:\nUpload a picture of something you like and find something similar. Select a picture and modify some attributes, for example \u0026ldquo;longer sleeves\u0026rdquo;, \u0026ldquo;change the neckline\u0026rdquo;, etc. After you find what you want, buy it directly from any mobile device. ","image":"https://mlgal.github.io/images/chatbot_screenshot.jpg","permalink":"https://mlgal.github.io/blogs/post-14/","tags":["Chatbot","Image Processing","Image Recognition","Deep Learning","Visual Search"],"title":"Get the Look on our Chatbot with Visual Search"},{"categories":[],"contents":"I gave a talk on how to use Deep Learning to search and serve relevant products to your web visitors.\nThe fashion industry is a visual world. Millions of images are displayed everyday by fashion commerce sites to serve consumers the latest trends and products. However, automatically categorizing and searching through large collections of images according to fine-grained attributes remains a challenge. In this talk we present our research on deep learning techniques to automatically identify fine-grained attributes in both images and text in the presence of incomplete and noisy data. We focus on attributes such as necklines, skirt and sleeve shapes, patterns, textures, colors and occasions. This task is especially useful to online stores who might want to automatically organize and mine visual items according to their attributes without human input. It is also useful for end users who wish to find specific items when there is no text available describing the target image. We compare the results of the deep learning approach with classical techniques such as Latent Dirichlet Allocation and Canonical Correlation Analysis. Our results show that it is possible to design algorithms that automatically “translate” visual concepts into text and vice-versa.\n","image":"https://mlgal.github.io/images/rework.jpg","permalink":"https://mlgal.github.io/blogs/post-12/","tags":["Talk","Contrastive Learning","Object Detection","Deep Learning"],"title":"Talk at ReWork: Deep Learning for Fashion Attributes Search"},{"categories":["Image Processing","Deep Learning"],"contents":"At Macty, we developed a novel in-store experience, where users can upload images of outfits and we suggest lingerie to pair it with.\nOur ‘Complete the Look’ application inspires customers while helping sellers increase the findability of their products and stimulate sales.”\nWe use advanced image recognition to detect visual attributes from outfits and lingerie to pair them together.\nHere\u0026rsquo;s a demo of what this looks like:\n","image":"https://mlgal.github.io/images/macty_vdv.jpg","permalink":"https://mlgal.github.io/blogs/post-15/","tags":["Deep Learning","Retailers","Image Processing","Fashion","Lingerie"],"title":"Transforming the in-store shopping experience with AI"},{"categories":["Deep Learning","Convolutional Neural Network","Image Recognition","Topic Model","NLP"],"contents":"In this paper we develop a neural network which learns inter- modal representations for fashion attributes to be utilized in a cross-modal search tool. Our neural network learns from organic e-commerce data, which is characterized by clean image material, but noisy and incomplete product descrip- tions. First, we experiment with techniques to segment e- commerce images and their product descriptions into respec- tively image and text fragments denoting fashion attributes. Here, we propose a rule-based image segmentation approach which exploits the cleanness of e-commerce images. Next, we design an objective function which encourages our model to induce a common embedding space where a semantically related image fragment and text fragment have a high in- ner product. This objective function incorporates similarity information of image fragments to obtain better intermodal representations. A key insight is that similar looking image fragments should be described with the same text fragments. We explicitly require this in our objective function, and as such recover information which was lost due to noise and in- completeness in the product descriptions. We evaluate the inferred intermodal representations in cross-modal search. We demonstrate that the neural network model trained with our objective function on image fragments acquired with our rule-based segmentation approach improves the results of image search with textual queries by 198% for recall@1 and by 181% for recall@5 compared to results obtained by a state-of-the-art image search system on the same benchmark dataset.\nCheck out our paper Cross-modal Search for Fashion Attributes\nKDD Machine Learning Meets Fashion Workshop, 2017\nKatrien Laenen, Susana Zoghbi, Sien Moens\nPDF\n","image":"https://mlgal.github.io/images/inner_product.jpg","permalink":"https://mlgal.github.io/blogs/post-4/","tags":["Deep Learning","Convolutional Neural Network","Image Recognition","Topic Model","NLP"],"title":"Cross-modal Search for Fashion Attributes"},{"categories":["Deep Learning","Convolutional Neural Network","Image Recognition","Topic Model","NLP"],"contents":"Automatic linking of online content improves navigation possibilities for end users. We focus on linking content generated by users to other relevant sites. In particular, we study the problem of linking information between different usages of the same language, e.g., colloquial and formal idioms or the language of consumers versus the language of sellers. The challenge is that the same items are described using very distinct vocabularies. As a case study, we investigate a new task of linking textual Pinterest.com pins (colloquial) to online webshops (formal). We evaluate three different modeling paradigms based on probabilistic topic modeling: monolingual latent Dirichlet allocation (LDA), bilingual LDA (BiLDA) and a novel multi-idiomatic LDA model (MiLDA). We compare these to the unigram model with Dirichlet prior. Our results for all three topic models reveal the usefulness of modeling the hidden thematic structure of the data through topics. Our proposed MiLDA model is able to deal with intrinsic multi-idiomatic data by considering the shared vocabulary between the aligned document pairs.\nCheck out our paper Latent Dirichlet Allocation for Linking User-Generated Content and e-Commerce Data\nInformation Sciences, 2016\nSusana Zoghbi, Ivan Vulic, Sien Moens\nPDF\n","image":"https://mlgal.github.io/images/sigir_img.png","permalink":"https://mlgal.github.io/blogs/post-2/","tags":["Deep Learning","Convolutional Neural Network","Image Recognition","Topic Model","NLP"],"title":"Latent Dirichlet Allocation for Linking User-Generated Content and e-Commerce Data"},{"categories":["Deep Learning","Convolutional Neural Network","Image Recognition","Topic Model","NLP"],"contents":"We study the problem of linking information between different idiomatic usages of the same language, for example, colloquial and formal language. We propose a novel probabilistic topic model called multi-idiomatic LDA (MiLDA). Its modeling principles follow the intuition that certain words are shared between two idioms of the same language, while other words are non-shared. We demonstrate the ability of our model to learn relations between cross-idiomatic topics in a dataset containing product descriptions and reviews. We present the utility of the new MiLDA topic model in a recently proposed information retrieval task of linking Pinterest pins to online webshops . We show that our multi-idiomatic model outperforms the standard monolingual LDA model and the pure bilingual LDA model both in terms of perplexity and MAP scores in the IR task.\nCheck out our paper Learning to Bridge Colloquial and Formal Language Applied to Linking and Search of E-Commerce Data\nACM SIGIR Conference on Research \u0026amp; Development in Information Retrieval (SIGIR \u0026lsquo;14)\nIvan Vulic, Susana Zoghbi and Sien Moens\nPDF\n","image":"https://mlgal.github.io/images/inf_science.png","permalink":"https://mlgal.github.io/blogs/post-3/","tags":["Deep Learning","Convolutional Neural Network","Image Recognition","Topic Model","NLP"],"title":"Learning to Bridge Colloquial and Formal Language Applied to Linking and Search of E-Commerce Data"},{"categories":["Deep Learning","Convolutional Neural Network","Image Recognition","NLP"],"contents":"We propose to infer user interests on social media where multi-modal data (text, image etc.) exist. We leverage user-generated data from Pinterest.com as a natural expression of users’ interests. Our main contribution is exploiting a multi-modal space composed of images and text. This is a natural approach since humans express their interests with a combination of modalities. We performed experiments using the state-of-the-art image and textual representations, such as convolutional neural networks, word embeddings, and bags of visual and textual words. Our experimental results show that in fact jointly processing image and text increases the overall interest classification accuracy, when compared to uni-modal representations (i.e., using only text or using only images).\nCheck out our paper Inferring User Interests on Social Media From Text and Images\nInternational Workshop on Social Media Retrieval and Analysis in conjunction with the IEEE International Conference on Data Mining (ICDM 2015)\nYagmur Gizem Cinar, Susana Zoghbi, Marie-Francine Moens\nPDF\n","image":"https://mlgal.github.io/images/somera.png","permalink":"https://mlgal.github.io/blogs/post-6/","tags":["Deep Learning","Convolutional Neural Network","Image Recognition","NLP"],"title":"Inferring User Interests on Social Media From Text and Images"},{"categories":["Deep Learning","Convolutional Neural Network","Image Recognition","NLP"],"contents":"In this paper we show an online demo that allows bidrectional multimodal queries for garments.\nCheck out our paper Cross-Modal Fashion Search\nIn Lecture Notes in Computer Science (LNCS) Vol. 9517, pp 367-373, 2016\nSusana Zoghbi, Geert Heyman, Juan Carlos Gomez, Sien Moens\nPDF\n","image":"https://mlgal.github.io/images/demo_img2txt.png","permalink":"https://mlgal.github.io/blogs/post-5/","tags":["Deep Learning","Convolutional Neural Network","Image Recognition","NLP"],"title":"Cross-Modal Fashion Search"},{"categories":["Topic Model"],"contents":"User-generated content offers opportunities to learn about people’s interests and hobbies. We can leverage this infor- mation to help users find interesting shops and businesses find interested users. However this content is highly noisy and unstructured as posted on social media sites and blogs. In this work we evaluate different textual representations and retrieval models that aim to make sense of social media data for retail applications. Our task is to link the text of pins (from Pinterest.com) to online shops (formed by cluster- ing Amazon.com’s products). Our results show that docu- ment representations that combine latent concepts with sin- gle words yield the best performance.\nCheck out our paper Are words enough?: a study on text-based representations and retrieval models for linking pins to online shops.\nProceedings of the 2013 International Workshop on Mining Unstructured Big Data using Natural Language Processing in conjunction with The 24th ACM International Conference on Information and Knowledge Management (CIKM 2013)\nIvan Vulic, Susana Zoghbi and Sien Moens\nPDF\n","image":"https://mlgal.github.io/images/graph.jpg","permalink":"https://mlgal.github.io/blogs/post-7/","tags":["Topic Model","LDA"],"title":"Are words enough?: a study on text-based representations and retrieval models for linking pins to online shops"},{"categories":["Robotics","Control","Human Robot Interaction"],"contents":"To effectively interact with people in a physically assistive role, robots will need to be able to cooperatively manipulate objects with a human partner. For example, it can be very difficult for an individual to manipulate a long or heavy object. An assistant can help to share the load, and improve the maneuverability of the object. Each partner can communicate objectives (e.g., move around an obstacle or put the object down) via non-verbal cues (e.g., moving the end of the object in a particular direction, changing speed, or tugging). Herein, non-verbal communication in a human-robot coordi- nated manipulation task is addressed using a small articulated robot arm equipped with a 6-axis wrist mounted force/torque sensor and joint angle encoders. The robot controller uses a Jacobian Transpose velocity PD control scheme with gravity compensation. To aid collaborative manipulation we implement a uniform impedance controller at the robot end-effector with an attractive force to a virtual path in the style of a cobot. Unlike a cobot, this path is recomputed online as a function of user input. In our present research, we utilize force/torque sensor measurements to identify intentional user communications specifying a change in the task direction. We consider the impact of path recomputation and the resulting robot haptic feedback on user physiological response.\nCheck out our paper Measuring Intent in Human-Robot Cooperative Manipulation\nProceedings of Workshop on Multimodal Human–Robot Interfaces, 2010 IEEE International Conference on Robotics and Automation (ICRA 2010)\nDavide De Carli, Evan Hohert, Chris AC Parker, Susana Zoghbi, Simon Leonard, Elizabeth Croft, Antonio Bicchi\nPDF\n","image":"https://mlgal.github.io/images/control.jpg","permalink":"https://mlgal.github.io/blogs/post-9/","tags":["Robotics","Control","Human Robot Interaction"],"title":"Measuring Intent in Human-Robot Cooperative Manipulation"}]